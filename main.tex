\documentclass[a4paper, twoside]{report}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage[hyphens]{url}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}

\setlength{\marginparwidth}{2cm}
\usepackage[colorinlistoftodos]{todonotes}

% adds newline, removes autoindent for new paragraphs
\usepackage{parskip}

% allows tables to be centred when going past text margins
\usepackage{adjustbox}

% increase height of each row in table relative to default height
\renewcommand{\arraystretch}{1.2}

% use subfigures
\usepackage{subcaption}

% typesetting algorithms
\usepackage{algorithm} 
\usepackage{algpseudocode}

% Change \Forall so that it prints 'for each' in algorithms
\renewcommand{\algorithmicforall}{\textbf{for each}}

% checks for unused references
%\usepackage{refcheck}

\title{Detecting Political Bias On Reddit With Transfer Learning}
\author{Fawaz Shah}
% Update supervisor and other title stuff in title/title.tex

\begin{document}
\input{0-title.tex}

\begin{abstract}

Identifying political bias on social media provides useful insights for analysts who want to quantify political sentiment online. However, tackling this task with traditional supervised machine learning is incredibly difficult due to the lack of annotated data available, unlike related tasks such as bias detection in news.

In this project we explore using transfer learning to detect political bias on social media, with knowledge learnt from detecting bias in news content. Specifically we look at unsupervised domain adaptation - a class of techniques developed for when no labelled data exists for the target problem. We investigate `direct' transfer learning, followed by a state-of-the-art domain-adaptive version of BERT called AdaptaBERT. We then propose an extension to AdaptaBERT involving an extra Next Sentence Prediction fine-tuning stage, and evaluate this on a political bias detection task and a named entity recognition (NER) task. We find this yields a 3.3\% improvement in F1 score for the NER task.

Significant further work is needed to help transfer learning classifiers reach the same performance as standard supervised classifiers, with the hope that they can exceed performance in the future.

\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}

Firstly I would like to thank my supervisor Dr Anandha Gopalan for his continuous guidance, enthusiasm and support throughout the project, during what has been a tough year.

Secondly, I want to thank everyone who I've had the pleasure of being friends with in the last 4 years. I especially owe a great deal to Anindita Ghosh, Subhash Nalluru, and Inara Ramji for proof-reading my report, as well as to Arjun, Zay and the rest of the \textit{Lit Times} squad for keeping me motivated through this pandemic.

Finally I want to thank my amazing family for their endless love and encouragement through my entire degree. I wouldn't be where I am today without them.

\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\input{1-introduction}
\input{2-background}
\input{3-ensemble-bert}
\input{4-reddit-data}
\input{5-domain-adaptation}
\input{6-extending-adaptabert}
\input{7-conclusion}
\input{8-bibliography}


\end{document}