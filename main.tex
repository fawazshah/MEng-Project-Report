\documentclass[a4paper, twoside]{report}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage[hyphens]{url}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}

\setlength{\marginparwidth}{2cm}
\usepackage[colorinlistoftodos]{todonotes}

% adds newline, removes autoindent for new paragraphs
\usepackage{parskip}

% allows tables to be centred when going past text margins
\usepackage{adjustbox}

% increase height of each row in table relative to default height
\renewcommand{\arraystretch}{1.2}

% use subfigures
\usepackage{subcaption}

% typesetting algorithms
\usepackage{algorithm} 
\usepackage{algpseudocode}

% Change \Forall so that it prints 'for each' in algorithms
\renewcommand{\algorithmicforall}{\textbf{for each}}

% checks for unused references
%\usepackage{refcheck}

\title{Detecting Political Bias On Reddit With Transfer Learning}
\author{Fawaz Shah}
% Update supervisor and other title stuff in title/title.tex

\begin{document}
\input{0-title.tex}

\begin{abstract}

Identifying political bias on social media provides useful insights for analysts who want to quantify political sentiment online, and explore problems on social media such as filter bubbles and echo chambers. However, tackling this task with traditional supervised machine learning is incredibly difficult due to the lack of annotated data available.

In this project we explore using transfer learning to detect political bias on social media, with knowledge learnt from bias in news content. We specifically look at unsupervised domain adaptation - a class of techniques developed specifically for when no labelled data exists for the target problem. We explore `direct' transfer learning followed by a state-of-the-art domain-adaptive version of BERT called AdaptaBERT, finding these do not improve over standard classifiers. We then propose an extension to AdaptaBERT involving an extra Next Sentence Prediction fine-tuning stage. We find this does not improve classification performance for the political bias detection task, but provides a 2.5\% improvement in F1 score for a named entity recognition task.

Significant further work is needed to help transfer learning classifiers reach the same performance as standard supervised classifiers, with the hope that they can exceed performance in the future.

\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}

Firstly I would like to thank my supervisor Dr Anandha Gopalan for his continuous guidance, enthusiasm and support throughout the project, during what has been a tough year.

Further, I want to thank everyone who I've had the pleasure of being friends with in the last 4 years. I especially owe a great deal to Andy, Subhash, and Inara for proof-reading my report, as well as Arjun, ZÃ©lie and the rest of the \textit{Lit Times} squad for keeping me motivated through this pandemic.

Finally I want to thank my amazing family for their endless love and encouragement through my entire degree. I wouldn't be where I am today without them.

\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\input{1-introduction}
\input{2-background}
\input{3-ensemble-bert}
\input{4-reddit-data}
\input{5-domain-adaptation}
\input{6-extending-adaptabert}
\input{7-conclusion}
\input{8-bibliography}


\end{document}