\chapter{Conclusion \& Future Work}

\section{Conclusion}

In this project we set out to explore if transfer learning, specifically domain adaptation, can help detect political bias on social media, using knowledge learnt from detecting the same bias in news content. We started by surveying classifiers to detect bias in the news, finding SVMs and gradient-boosted forests to give the best performance, using features from earlier research. We then examined BERT models for the same task, which perform feature extraction and classification holistically, finding these also give fairly good performance.

Using our self-developed cross-domain dataset of news articles and related Reddit comments, we explored unsupervised domain adaptation techniques for detecting political bias in news and social media comments. We found classification performance when using social media comments as a predictor for news bias (\textit{comments $ \rightarrow $ articles} transfer) significantly outperforms the reverse scenario, however in both cases transfer learning cannot outperform standard in-domain classifiers. We also found that a state-of-the-art domain adaptive BERT model called AdaptaBERT outperforms direct transfer learning when optimising for the right mix of source and target domain material during training, coming very close to matching the performance of in-domain classifiers.

We investigated extending AdaptaBERT with an extra Next Sentence Prediction stage, originally used in BERT pre-training, to see if this improves classification performance. This extension improves performance for an NER task originally explored in the AdaptaBERT paper \cite{adaptabert}, however this extension also results in performance deterioration for the original political bias detection task, possibly due to the differences between the source and target domains in terms of number of sentences per sample.

Significant work still needs to be done in the field of unsupervised domain adaptation to fully match the performance of in-domain classifiers, and to see if in-domain performance can be exceeded. This would aid political bias detection on social media tremendously, where supervised domain adaptation cannot be performed due to the lack of sufficient annotated data.

\section{Ethical Issues}

The output of our models are not sensitive data. They only predict a political class (e.g. left, center or right) for particular news articles or comments. 

Our project doesn't involve collecting any personal data from Reddit users - all information is collected through the public Reddit API. We don't make use of author information from comments in any way. The API doesn't expose any sensitive data anyway, only username and account age are exposed.



There are a number of ethical issues involved in our research.

Someone could take our bias detectors and use them on comments all from one person to determine that person's political leaning.

In using Reddit comments as a predictor for political bias in the wider news context, our model may unknowingly be biased towards writing styles used by the demographics in our dataset. Reddit is known to have a mainly young, American, white and male userbase, and so a bias detection classifier trained on Reddit text such as ours may start to mispredict if it comes across news or comments with writing styles different to those used by this demographic online. 

This risk increases with large language models such as BERT, which are pre-trained on huge amounts of data. 

Reddit demographics


https://www.journalism.org/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/

work has focused on examining data that feeds into large language models such as BERT


wanted to find bias per user, but data protection stuff

taking biases learnt from comments and transferring them to news source detection - unneeded influence


For all information we collect we must abide by the Data Protection Act (DPA) 2018 \cite{dpa}, which sets out UK GDPR rules. In this project we do not manually collect any private data relating to Reddit users - all data we use is exposed publicly by the Reddit API. However for all comments we collect, the author's Reddit username is also collected as part of the API response. This legally counts as storing sensitive personal information under the DPA, however we mitigate this issue by discarding Reddit usernames from our dataset as soon as possible, since it is not relevant to our research. We don't infer any political bias on a per-user basis, however if we did we would have to be careful not to publish this association anywhere (for obvious reasons).

For others to use our model in production systems we would need to verify our models are unbiased. Machine learning models are often black-box models - verifying what behaviour occurs inside ML models is an open research topic, and so we can never say for sure that our models are completely unbiased. We could however compare against manually-annotated data to see where bias in our models might lie.


biased data -> biased model

gebru, reddit
https://dl.acm.org/doi/abs/10.1145/3351095.3372829


\section{Future Work}

Exploring bias in tabloids vs broadsheets

Further work on Reddit:
\begin{itemize}
    \item Compare Reddit political scale to various countries' political scales (e.g. does it most align with the US scale?)
    \item Look at bias across time
\end{itemize}

Explore effect of NSP addition to AdaptaBERT for other problems (question answering, natural language inference)

