\chapter{Conclusion, Evaluation, Ethical Issues \& Future Work}

\section{Conclusion}

In this project we set out to explore if transfer learning, specifically domain adaptation, can help detect political bias on social media, using knowledge learnt from detecting the same bias in news content. We started by surveying classifiers to detect bias in the news, finding SVMs and gradient-boosted forests to give the best performance, using features from earlier research. We then examined BERT models for the same task, which perform feature extraction and classification holistically, finding these also give fairly good performance.

For our domain adaptation task, we created our own dataset of news articles and Reddit comments annotated by political bias, making sure to preserve data quality by ensuring high-quality annotations, avoiding concept drift and resolving class imbalance. Examining similarities between the domains in our dataset, we found fairly high similarity between news article and comment text, lending weight to the hypothesis that domain adaptation can improve bias detection in these domains.

We then explored a direct transfer approach and an approach using AdaptaBERT for unsupervised domain adaptation from news content to social media content, and also from social media content to news. We found that in both cases domain adaptation cannot quite reach the classification performance of standard in-domain models, however the transfer from comments to news was able to reach within 7\% of the F1 score of an in-domain model. This signifies that readers' reactions to news articles can be a good predictor of the bias of the article itself.

We also found AdaptaBERT does not significantly improve over simple direct transfer. We investigate extending AdaptaBERT with an extra Next Sentence Prediction stage, originally used in BERT pre-training, to see if this improves classification performance. We find that it improves F1 score by 8\% in news to comments transfer, and by around 1\% for the reverse transfer. However, it still doesn't beat the in-domain baselines. For a named entity recognition task, it improves on F1 score by around 3\% compared to previous research. We have therefore achieved a significant improvement over standard AdaptaBERT, although there is still scope for much further work in this field.

\section{Evaluation}

Reddit subreddit titles (which form our annotations) all follow US political scale roughly

\section{Ethical Issues}


wanted to find bias per user, but data protection stuff

taking biases learnt from comments and transferring them to news source detection - unneeded influence


\todo{write an up to date version}

For all information we collect we must abide by the Data Protection Act (DPA) 2018 \cite{dpa}, which sets out UK GDPR rules. In this project we do not manually collect any private data relating to Reddit users - all data we use is exposed publicly by the Reddit API. However for all comments we collect, the author's Reddit username is also collected as part of the API response. This legally counts as storing sensitive personal information under the DPA, however we mitigate this issue by discarding Reddit usernames from our dataset as soon as possible, since it is not relevant to our research. We don't infer any political bias on a per-user basis, however if we did we would have to be careful not to publish this association anywhere (for obvious reasons).

For others to use our model in production systems we would need to verify our models are unbiased. Machine learning models are often black-box models - verifying what behaviour occurs inside ML models is an open research topic, and so we can never say for sure that our models are completely unbiased. We could however compare against manually-annotated data to see where bias in our models might lie.


biased data -> biased model

gebru, reddit
https://dl.acm.org/doi/abs/10.1145/3351095.3372829


\section{Future Work}

Exploring bias in tabloids vs broadsheets

Further work on Reddit:
\begin{itemize}
    \item Compare Reddit political scale to various countries' political scales (e.g. does it most align with the US scale?)
    \item Look at bias across time
\end{itemize}

Explore effect of NSP addition to AdaptaBERT for other problems (question answering, natural language inference)

